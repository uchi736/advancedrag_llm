import streamlit as st
import pandas as pd
import tempfile
import shutil
import asyncio
from pathlib import Path
from datetime import datetime
from sqlalchemy import text
from src.rag.term_extraction import JargonDictionaryManager
from src.rag.config import Config
from src.utils.helpers import render_term_card

def _get_available_collections(rag_system):
    """Get list of available collections from database"""
    try:
        with rag_system.engine.connect() as conn:
            result = conn.execute(text("""
                SELECT DISTINCT collection_name
                FROM document_chunks
                ORDER BY collection_name
            """))
            collections = [row[0] for row in result]
            return collections if collections else [rag_system.config.collection_name]
    except Exception as e:
        return [rag_system.config.collection_name]

@st.cache_data(ttl=60, show_spinner=False)
def get_all_terms_cached(_jargon_manager):
    return pd.DataFrame(_jargon_manager.get_all_terms())

def check_vector_store_has_data(rag_system):
    """Check if vector store or document chunks have any data."""
    try:
        if not rag_system or not hasattr(rag_system, 'engine'):
            return False

        with rag_system.engine.connect() as conn:
            # Check vector store (langchain_pg_embedding)
            try:
                result = conn.execute(text("SELECT COUNT(*) FROM langchain_pg_embedding"))
                vector_count = result.scalar()
            except:
                vector_count = 0

            # Check keyword search chunks (document_chunks)
            try:
                result = conn.execute(text("SELECT COUNT(*) FROM document_chunks"))
                chunk_count = result.scalar()
            except:
                chunk_count = 0

            # Return True if either table has data
            return vector_count > 0 or chunk_count > 0
    except Exception as e:
        import logging
        logging.error(f"Error checking vector store: {e}")
        return False

def render_dictionary_tab(rag_system):
    """Renders the dictionary tab."""
    st.markdown("### ğŸ“– å°‚é–€ç”¨èªè¾æ›¸")
    st.caption("ç™»éŒ²ã•ã‚ŒãŸå°‚é–€ç”¨èªãƒ»é¡ç¾©èªã‚’æ¤œç´¢ãƒ»ç¢ºèªãƒ»å‰Šé™¤ã§ãã¾ã™ã€‚")

    if not rag_system:
        st.warning("âš ï¸ RAGã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    # Collection selection UI (before getting jargon_manager)
    with st.expander("ğŸ“‚ å¯¾è±¡ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³", expanded=False):
        available_collections = _get_available_collections(rag_system)
        current_collection = st.session_state.get("selected_collection", rag_system.config.collection_name)

        selected_collection = st.selectbox(
            "å°‚é–€ç”¨èªæŠ½å‡ºã®å¯¾è±¡ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’é¸æŠ",
            available_collections,
            index=available_collections.index(current_collection) if current_collection in available_collections else 0,
            key="dictionary_collection_selector"
        )

        if (selected_collection and selected_collection != st.session_state.get("selected_collection")) or \
           st.session_state.get("force_collection_switch", False):
            st.session_state.selected_collection = selected_collection
            st.session_state.force_collection_switch = False  # Clear flag after processing
            if "rag_system" in st.session_state:
                del st.session_state["rag_system"]
            # Clear cached terms
            get_all_terms_cached.clear()
            st.rerun()

        st.info(f"**ç¾åœ¨ã®å¯¾è±¡:** {current_collection}")
        st.caption("ğŸ’¡ æ–°è¦ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ä½œæˆã¯ã€ŒğŸ“¤ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã€ã‚¿ãƒ–ã§è¡Œãˆã¾ã™")

    # Check if jargon manager is available (after collection switch handling)
    if not hasattr(rag_system, 'jargon_manager') or rag_system.jargon_manager is None:
        st.warning("âš ï¸ å°‚é–€ç”¨èªè¾æ›¸æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        return

    jargon_manager = rag_system.jargon_manager

    # Manual term registration form
    with st.expander("â• æ–°ã—ã„ç”¨èªã‚’æ‰‹å‹•ã§ç™»éŒ²ã™ã‚‹"):
        with st.form(key="add_term_form"):
            new_term = st.text_input("ç”¨èª*", help="ç™»éŒ²ã™ã‚‹å°‚é–€ç”¨èª")
            new_definition = st.text_area("å®šç¾©*", help="ç”¨èªã®å®šç¾©ã‚„èª¬æ˜")
            new_domain = st.text_input("åˆ†é‡", help="é–¢é€£ã™ã‚‹æŠ€è¡“åˆ†é‡ã‚„ãƒ‰ãƒ¡ã‚¤ãƒ³")
            new_aliases = st.text_input("é¡ç¾©èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: RAG, æ¤œç´¢æ‹¡å¼µç”Ÿæˆ")
            new_related_terms = st.text_input("é–¢é€£èª (ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š)", help="ä¾‹: LLM, Vector Search")
            
            submitted = st.form_submit_button("ç™»éŒ²")
            if submitted:
                if not new_term or not new_definition:
                    st.error("ã€Œç”¨èªã€ã¨ã€Œå®šç¾©ã€ã¯å¿…é ˆé …ç›®ã§ã™ã€‚")
                else:
                    aliases_list = [alias.strip() for alias in new_aliases.split(',') if alias.strip()]
                    related_list = [rel.strip() for rel in new_related_terms.split(',') if rel.strip()]
                    
                    if jargon_manager.add_term(
                        term=new_term,
                        definition=new_definition,
                        domain=new_domain,
                        aliases=aliases_list,
                        related_terms=related_list
                    ):
                        st.success(f"ç”¨èªã€Œ{new_term}ã€ã‚’ç™»éŒ²ã—ã¾ã—ãŸã€‚")
                        get_all_terms_cached.clear()
                        st.rerun()
                    else:
                        st.error(f"ç”¨èªã€Œ{new_term}ã€ã®ç™»éŒ²ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    
    st.markdown("---")

    # Search and refresh buttons
    col1, col2 = st.columns([3, 1])
    with col1:
        search_keyword = st.text_input(
            "ğŸ” ç”¨èªæ¤œç´¢",
            placeholder="æ¤œç´¢ã—ãŸã„ç”¨èªã‚’å…¥åŠ›ã—ã¦ãã ã•ã„...",
            key="term_search_input"
        )
    with col2:
        st.markdown("<br>", unsafe_allow_html=True)
        if st.button("ğŸ”„ æ›´æ–°", key="refresh_terms", use_container_width=True):
            get_all_terms_cached.clear()
            st.rerun()

    # Load term data
    with st.spinner("ç”¨èªè¾æ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­..."):
        all_terms_df = get_all_terms_cached(jargon_manager)

    # ç”¨èªç”ŸæˆUI - Always show at top
    st.markdown("### ğŸ“š ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆ")

    # Check vector store status
    has_vector_data = check_vector_store_has_data(rag_system)
    if not has_vector_data:
        st.warning("âš ï¸ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        st.info("""
ğŸ’¡ **äº‹å‰æº–å‚™ãŒå¿…è¦ã§ã™**:
1. ã€Œ**ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ**ã€ã‚¿ãƒ–ã§PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒ»ç™»éŒ²
2. ã“ã®ã‚¿ãƒ–ã«æˆ»ã£ã¦ç”¨èªã‚’ç”Ÿæˆ

å®šç¾©ç”Ÿæˆã¨LLMåˆ¤å®šã‚’æœ‰åŠ¹ã«ã™ã‚‹ã«ã¯ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç™»éŒ²ãŒå¿…é ˆã§ã™ã€‚
        """)
    else:
        st.success("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™ã€‚ç”¨èªç”Ÿæˆã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚")

    st.markdown("""
**ğŸ“š ç”¨èªè¾æ›¸ç”Ÿæˆã®æµã‚Œ**:
1. ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰å°‚é–€ç”¨èªå€™è£œã‚’æŠ½å‡º (LLMãƒ™ãƒ¼ã‚¹)
2. çœŸã®å°‚é–€ç”¨èªã‚’é¸åˆ¥
3. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‹ã‚‰é–¢é€£æ–‡æ›¸ã‚’æ¤œç´¢ã—ã¦å®šç¾©ç”Ÿæˆ
4. å€™è£œãƒ—ãƒ¼ãƒ«å…¨ä½“ã‹ã‚‰é¡ç¾©èªã‚’æ¤œå‡º
    """)

    # Input mode selection
    input_mode = st.radio(
        "å…¥åŠ›ã‚½ãƒ¼ã‚¹",
        ("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º", "æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"),
        horizontal=True,
        key="term_input_mode"
    )

    uploaded_files = None
    input_dir = ""
    if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
        st.info("ç™»éŒ²æ¸ˆã¿ã®å…¨ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ç”¨èªã‚’æŠ½å‡ºã—ã¾ã™ã€‚")
        input_dir = "./docs"  # Placeholder, will use vector store docs
    else:
        uploaded_files = st.file_uploader(
            "ç”¨èªæŠ½å‡ºç”¨ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ (PDFæ¨å¥¨)",
            accept_multiple_files=True,
            type=["pdf", "txt", "md"],
            key="term_input_files"
        )

    output_json = st.text_input(
        "å‡ºåŠ›å…ˆ (JSON)",
        value="./output/terms.json",
        key="term_output_json"
    )

    if st.button("ğŸš€ ç”¨èªã‚’æŠ½å‡ºãƒ»ç”Ÿæˆ", type="primary", use_container_width=True, key="run_term_extraction", disabled=not has_vector_data):
        if not hasattr(rag_system, 'jargon_manager') or rag_system.jargon_manager is None:
            st.error("ç”¨èªè¾æ›¸æ©Ÿèƒ½ã¯ç¾åœ¨åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        else:
            temp_dir_path = None
            try:
                if input_mode == "ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰æŠ½å‡º":
                    # Extract text from registered documents in database
                    with rag_system.engine.connect() as conn:
                        result = conn.execute(text("""
                            SELECT content
                            FROM document_chunks
                            WHERE collection_name = :collection_name
                            ORDER BY created_at
                        """), {"collection_name": rag_system.config.collection_name})
                        all_chunks = [row[0] for row in result]

                    if not all_chunks:
                        st.error("ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
                    else:
                        # Create temporary file with all content
                        temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_registered_"))
                        temp_file = temp_dir_path / "registered_documents.txt"

                        # Write all chunks to file
                        with open(temp_file, "w", encoding="utf-8") as f:
                            f.write("\n\n".join(all_chunks))

                        input_path = str(temp_dir_path)
                        st.info(f"ç™»éŒ²æ¸ˆã¿ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰ {len(all_chunks)} ãƒãƒ£ãƒ³ã‚¯ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")

                        output_path = Path(output_json)
                        output_path.parent.mkdir(parents=True, exist_ok=True)

                        # Stage 2.5å¯è¦–åŒ–ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æº–å‚™
                        stage25_placeholder = st.empty()

                        def ui_callback(event_type, data):
                            """Stage 2.5ã‚¤ãƒ™ãƒ³ãƒˆã‚’UIã«è¡¨ç¤º"""
                            if event_type == "stage25_reflection":
                                with stage25_placeholder.container():
                                    st.markdown(f"### ğŸ¤” åå¾© {data['iteration']}/{data['max_iterations']} - è‡ªå·±åçœ")
                                    col1, col2, col3 = st.columns(3)
                                    col1.metric("ä¿¡é ¼åº¦", f"{data['confidence']:.2f}")
                                    col2.metric("å•é¡Œç‚¹", f"{len(data['issues'])}å€‹")
                                    col3.metric("æ¼ã‚Œç”¨èª", f"{len(data['missing'])}å€‹")

                                    if data['issues']:
                                        with st.expander("æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ", expanded=False):
                                            for issue in data['issues'][:5]:  # ä¸Šä½5ä»¶
                                                st.markdown(f"- {issue}")

                                    if data['missing']:
                                        with st.expander("æ¼ã‚Œç”¨èªå€™è£œ", expanded=False):
                                            st.markdown(", ".join(data['missing'][:10]))  # æœ€å¤§10ä»¶

                            elif event_type == "stage25_action":
                                with stage25_placeholder.container():
                                    if data['removed'] > 0 or data['added'] > 0:
                                        st.success(f"âœ… é™¤å¤–: {data['removed']}å€‹ã€è¿½åŠ : {data['added']}å€‹")

                        with st.status("ç”¨èªæŠ½å‡ºä¸­...", expanded=True) as status:
                            # Get latest rag_system from session state to ensure correct collection_name
                            current_rag = st.session_state.get("rag_system", rag_system)
                            asyncio.run(current_rag.extract_terms(input_path, str(output_path), ui_callback=ui_callback))
                            status.update(label="âœ… æŠ½å‡ºå®Œäº†!", state="complete")

                        st.session_state['term_extraction_output'] = str(output_path)
                        st.success(f"âœ… ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ â†’ {output_path}")
                        get_all_terms_cached.clear()
                        st.rerun()
                else:
                    if not uploaded_files:
                        st.error("æŠ½å‡ºã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ã€‚")
                    else:
                        temp_dir_path = Path(tempfile.mkdtemp(prefix="term_extract_"))
                        for uploaded in uploaded_files:
                            target = temp_dir_path / uploaded.name
                            with open(target, "wb") as f:
                                f.write(uploaded.getbuffer())
                        input_path = str(temp_dir_path)

                        output_path = Path(output_json)
                        output_path.parent.mkdir(parents=True, exist_ok=True)

                        # Stage 2.5å¯è¦–åŒ–ç”¨ã®ã‚³ãƒ³ãƒ†ãƒŠã‚’æº–å‚™
                        stage25_placeholder = st.empty()

                        def ui_callback(event_type, data):
                            """Stage 2.5ã‚¤ãƒ™ãƒ³ãƒˆã‚’UIã«è¡¨ç¤º"""
                            if event_type == "stage25_reflection":
                                with stage25_placeholder.container():
                                    st.markdown(f"### ğŸ¤” åå¾© {data['iteration']}/{data['max_iterations']} - è‡ªå·±åçœ")
                                    col1, col2, col3 = st.columns(3)
                                    col1.metric("ä¿¡é ¼åº¦", f"{data['confidence']:.2f}")
                                    col2.metric("å•é¡Œç‚¹", f"{len(data['issues'])}å€‹")
                                    col3.metric("æ¼ã‚Œç”¨èª", f"{len(data['missing'])}å€‹")

                                    if data['issues']:
                                        with st.expander("æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ", expanded=False):
                                            for issue in data['issues'][:5]:  # ä¸Šä½5ä»¶
                                                st.markdown(f"- {issue}")

                                    if data['missing']:
                                        with st.expander("æ¼ã‚Œç”¨èªå€™è£œ", expanded=False):
                                            st.markdown(", ".join(data['missing'][:10]))  # æœ€å¤§10ä»¶

                            elif event_type == "stage25_action":
                                with stage25_placeholder.container():
                                    if data['removed'] > 0 or data['added'] > 0:
                                        st.success(f"âœ… é™¤å¤–: {data['removed']}å€‹ã€è¿½åŠ : {data['added']}å€‹")

                        with st.status("ç”¨èªæŠ½å‡ºä¸­...", expanded=True) as status:
                            # Get latest rag_system from session state to ensure correct collection_name
                            current_rag = st.session_state.get("rag_system", rag_system)
                            asyncio.run(current_rag.extract_terms(input_path, str(output_path), ui_callback=ui_callback))
                            status.update(label="âœ… æŠ½å‡ºå®Œäº†!", state="complete")

                        st.session_state['term_extraction_output'] = str(output_path)
                        st.success(f"âœ… ç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã—ã¾ã—ãŸ â†’ {output_path}")
                        get_all_terms_cached.clear()
                        st.rerun()

            except Exception as e:
                st.error(f"ç”¨èªæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
                import traceback
                st.code(traceback.format_exc())
            finally:
                if temp_dir_path and temp_dir_path.exists():
                    shutil.rmtree(temp_dir_path, ignore_errors=True)

    # ç”¨èªæŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼
    output_file = st.session_state.get('term_extraction_output', '')
    if output_file and Path(output_file).exists():
        st.markdown("---")
        with st.expander("ğŸ“Š æŠ½å‡ºçµæœã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", expanded=False):
            import json
            try:
                with open(output_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    terms = data.get('terms', [])

                st.success(f"âœ… {len(terms)}ä»¶ã®ç”¨èªã‚’æŠ½å‡ºã—ã¾ã—ãŸ")

                # ä¸Šä½10ä»¶ã‚’è¡¨ç¤º
                st.markdown("**ä¸Šä½10ä»¶ã®ç”¨èª:**")
                for i, term in enumerate(terms[:10], 1):
                    with st.container():
                        col1, col2 = st.columns([3, 1])
                        with col1:
                            st.markdown(f"**{i}. {term['headword']}**")
                            if term.get('definition'):
                                st.caption(term['definition'][:100] + "..." if len(term['definition']) > 100 else term['definition'])
                        with col2:
                            st.metric("ã‚¹ã‚³ã‚¢", f"{term.get('score', 0):.3f}")
                            st.caption(f"é »åº¦: {term.get('frequency', 0)}")

            except Exception as e:
                st.error(f"çµæœãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # ãƒŠãƒ¬ãƒƒã‚¸ã‚°ãƒ©ãƒ•æ©Ÿèƒ½ã¯å‰Šé™¤ã•ã‚Œã¾ã—ãŸ
    st.markdown("---")

    # Show registered terms section
    if all_terms_df.empty:
        st.info("ã¾ã ç”¨èªãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ä¸Šè¨˜ã®ã€Œç”¨èªè¾æ›¸ã‚’ç”Ÿæˆã€ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        return

    # Filter terms
    if search_keyword:
        terms_df = all_terms_df[
            all_terms_df['term'].str.contains(search_keyword, case=False) |
            all_terms_df['definition'].str.contains(search_keyword, case=False) |
            all_terms_df['aliases'].apply(lambda x: any(search_keyword.lower() in str(s).lower() for s in x) if x else False)
        ]
    else:
        terms_df = all_terms_df

    if terms_df.empty:
        st.info(f"ã€Œ{search_keyword}ã€ã«è©²å½“ã™ã‚‹ç”¨èªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
        return

    # Statistics
    col1, col2 = st.columns(2)
    with col1:
        st.metric("ç™»éŒ²ç”¨èªæ•°", f"{len(terms_df):,}")
    with col2:
        total_synonyms = sum(len(syn_list) if syn_list else 0 for syn_list in terms_df['aliases'])
        st.metric("é¡ç¾©èªç·æ•°", f"{total_synonyms:,}")

    st.markdown("---")

    # View mode selection
    view_mode = st.radio(
        "è¡¨ç¤ºå½¢å¼",
        ["ã‚«ãƒ¼ãƒ‰å½¢å¼", "ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼"],
        horizontal=True,
        key="dict_view_mode"
    )

    if view_mode == "ã‚«ãƒ¼ãƒ‰å½¢å¼":
        # Batch render to reduce WebSocket calls
        for idx, row in terms_df.iterrows():
            with st.container():
                render_term_card(row)
                # Use hash for stable keys to prevent WebSocket errors
                import hashlib
                term_hash = hashlib.md5(f"{row['term']}_{idx}".encode()).hexdigest()[:8]
                delete_key = f"delete_card_{term_hash}"
                if st.button("å‰Šé™¤", key=delete_key, use_container_width=True):
                    deleted, errors = rag_system.delete_jargon_terms([row['term']])
                    if deleted:
                        st.success(f"ç”¨èªã€Œ{row['term']}ã€ã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                        get_all_terms_cached.clear()
                        st.rerun()
                    else:
                        st.error(f"ç”¨èªã€Œ{row['term']}ã€ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

    else: # ãƒ†ãƒ¼ãƒ–ãƒ«å½¢å¼
        display_df = terms_df.copy()
        display_df['aliases'] = display_df['aliases'].apply(lambda x: ', '.join(x) if x else '')
        display_df['related_terms'] = display_df['related_terms'].apply(lambda x: ', '.join(x) if x else '')
        
        # ã‚«ãƒ©ãƒ åã‚’æ—¥æœ¬èªã«
        column_mapping = {
            'term': 'ç”¨èª', 'definition': 'å®šç¾©', 'domain': 'åˆ†é‡',
            'aliases': 'é¡ç¾©èª', 'related_terms': 'é–¢é€£èª',
            'updated_at': 'æ›´æ–°æ—¥æ™‚'
        }
        # Add 'id' mapping only if it exists
        if 'id' in display_df.columns:
            column_mapping['id'] = 'ID'
        display_df.rename(columns=column_mapping, inplace=True)

        # å‰Šé™¤ãƒœã‚¿ãƒ³ç”¨ã®åˆ—ã‚’è¿½åŠ 
        display_df['å‰Šé™¤'] = False

        edited_df = st.data_editor(
            display_df[['ç”¨èª', 'å®šç¾©', 'åˆ†é‡', 'é¡ç¾©èª', 'é–¢é€£èª', 'æ›´æ–°æ—¥æ™‚', 'å‰Šé™¤']],
            use_container_width=True,
            hide_index=True,
            height=min(600, (len(display_df) + 1) * 35 + 3),
            column_config={
                "å‰Šé™¤": st.column_config.CheckboxColumn(
                    "å‰Šé™¤",
                    default=False,
                )
            },
            key="dictionary_editor"
        )
        
        terms_to_delete = edited_df[edited_df['å‰Šé™¤']]
        if not terms_to_delete.empty:
            if st.button("é¸æŠã—ãŸç”¨èªã‚’å‰Šé™¤", type="primary"):
                terms_list = terms_to_delete['ç”¨èª'].tolist()
                deleted_count, error_count = rag_system.delete_jargon_terms(terms_list)
                if deleted_count:
                    st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
                if error_count:
                    st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
                get_all_terms_cached.clear()
                st.rerun()

    # CSV download
    st.markdown("---")
    with st.expander("âš ï¸ ç”¨èªè¾æ›¸ã‚’å…¨å‰Šé™¤ã™ã‚‹"):
        st.warning("ã“ã®æ“ä½œã¯å–ã‚Šæ¶ˆã›ã¾ã›ã‚“ã€‚å…¨ã¦ã®å°‚é–€ç”¨èªãƒ¬ã‚³ãƒ¼ãƒ‰ãŒå‰Šé™¤ã•ã‚Œã¾ã™ã€‚", icon="âš ï¸")
        if st.button("â€¼ï¸ å…¨ç”¨èªã‚’å‰Šé™¤", type="secondary"):
            deleted_count, error_count = rag_system.delete_jargon_terms(terms_df['term'].tolist())
            if deleted_count:
                st.success(f"{deleted_count}ä»¶ã®ç”¨èªã‚’å‰Šé™¤ã—ã¾ã—ãŸã€‚")
            if error_count:
                st.warning(f"{error_count}ä»¶ã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸã€‚", icon="âš ï¸")
            get_all_terms_cached.clear()
            st.rerun()
    csv = terms_df.to_csv(index=False)
    st.download_button(
        label="ğŸ“¥ è¡¨ç¤ºä¸­ã®ç”¨èªã‚’CSVã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
        data=csv,
        file_name=f"jargon_dictionary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
        mime="text/csv",
        key="csv_download_button"
    )
